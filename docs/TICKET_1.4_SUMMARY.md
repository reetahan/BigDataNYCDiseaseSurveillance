# Ticket 1.4: NYC 311 & Open Data Scraper - COMPLETE âœ…

**Assignee:** Devak Somaraj (ds8095)
**Status:** Complete
**Date Completed:** November 26, 2025

## Summary
Implemented a robust API client for the NYC 311 Service Requests dataset. This tool ingests environmental health indicators (Rodents, Sanitation, Food Poisoning) directly from the Socrata Open Data API. These indicators serve as "leading signals" for potential disease clusters (e.g., rat complaints predicting leptospirosis).

## Deliverables

### Core Implementation
1.  **311 Scraper** ([scrapers/scraper_311.py](scrapers/scraper_311.py))
    - `sodapy` client integration for reliable API interaction.
    - Server-side filtering using SoQL (Socrata Query Language).
    - Intelligent time-window management to handle data latency.
    - Nested JSON transformation for schema standardization.

2.  **Configuration** ([scrapers/config.py](scrapers/config.py))
    - Centralized settings for API endpoints and Dataset IDs.
    - Configurable lookback windows (default: 7 days).

### Infrastructure
1.  **Dependencies** ([requirements.txt](requirements.txt))
    - `sodapy==2.2.0`: Official Socrata API client.
    - `python-dotenv`: For secure token management.
    - `pandas`: For potential data frame manipulations (if needed later).

2.  **Security**
    - `NYC_APP_TOKEN` managed via environment variables to prevent hardcoding credentials.

## Technical Highlights

### 1. Server-Side Optimization (SoQL)
**Constraint:** The 311 dataset contains millions of records (noise complaints, parking tickets) which are irrelevant to disease surveillance. Downloading the full daily dump is inefficient.

**Solution:** We construct a SoQL query to filter data *before* it leaves NYC servers.
```sql
SELECT * WHERE created_date > '...' AND (complaint_type LIKE '%Rodent%' OR ...)
```
Impact: Reduces network bandwidth usage by ~99%.

### 2. Handling Data Latency
**Challenge:** We observed that NYC Open Data often has a reporting lag of 24-48 hours for non-emergency 311 tickets.

**Solution:** The scraper defaults to a 7-day lookback window (configurable).

**Logic:** It is safer to re-ingest overlapping data (which Layer 3 deduplicates) than to miss late-arriving records by using a strict 24-hour window.

### 3. "Anonymous" vs. "Authenticated" Modes
The script is designed to run in two modes:

- **Authenticated:** Uses NYC_APP_TOKEN for high throughput.

- **Anonymous:** Falls back to throttled limits if no token is present, ensuring the scraper works immediately for any teammate pulling the code.

## Output Format

Data is transformed from the flat API response to a nested schema to group geospatial data:

```json
[
  {
      "source": "NYC_311",
      "id": "5872139",
      "timestamp": "2025-11-23T09:00:00",
      "type": "Rodent",
      "description": "Rat sighting in restaurant basement",
      "location": {
          "zip": "11201",
          "lat": "40.69",
          "lon": "-73.98"
      },
      "status": "Open",
      "scraped_at": "2025-11-23T10:00:00"
  }
]
```

## Usage Examples
### Standard Run: 
```bash
python scraper_311.py
# Uses lookback window from config.py (7 days)
```
### Setup Credentials (Optional)
```bash
export NYC_APP_TOKEN="your_token_here"
python scraper_311.py
```

## Integration Points
1. **Input:** NYC Open Data API (Socrata).
2. **Output:** JSON files in data/nyc_311/.
3. **Next Stage:** Geo-spatial clustering in Layer 5 (detecting "dirty" neighborhoods).

## Known Limitations
1. **API Throttling:** Without a token, requests are limited.
2. **Data Quality:** 311 data is user-reported and may contain duplicates or false reports (requires downstream cleaning).

## Files Created
- scrapers/scraper_311.py
- scrapers/config.py
- TICKET_1.4_SUMMARY.md